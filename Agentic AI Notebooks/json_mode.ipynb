{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JSON Mode\n",
    "\n",
    "When building AI applications, especially agentic systems, you often need the model to generate more than just free-form text responses. Much more capable systems are possible when you can generate data in a format that your application can parse, process, and act upon. This is where JSON Mode comes in - a simple yet powerful technique for getting structured outputs from language models.\n",
    "\n",
    "In this lesson, you'll learn how to enable JSON Mode in the OpenAI API, how to prompt models effectively to produce valid JSON, and how to process JSON lists and dictionaries to build more complex workflows. By the end, you'll have practical experience with a fundamental technique used in many advanced AI applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's set up our environment with the necessary imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we dive into AI-generated JSON, let's do a super quick refresher on parsing JSON strings in Python.\n",
    "\n",
    "The `json_string` below is a string of valid JSON that needs to be converted into a Python dictionary before we can access the data inside. For this purpose, we import the `json` module and use its method `.loads()`, passing in the string. Execute the cell below to see it in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json_string = \"\"\"{{\"data\": [1,2,3]}}\"\"\"\n",
    "\n",
    "parsed_json = json.loads(json_string)\n",
    "print(parsed_json['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It sure would be nice if we could generate JSON with an LLM that we could parse using `.loads()` into valid Python, right?\n",
    "\n",
    "While models are getting better and better at following instructions, they do still mess this up if we don't use all the tools at our disposal. For instance, execute the following cell and see what the model outputs when we request the answer in JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=\"gpt-4o\"):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        seed=42,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "get_completion('Who won the world series in 2020? Please respond in JSON, with \"winner\" as the key and the winner as the value.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the problem? LLMs often default to adding markdown to outputs. As a result, our JSON is forced inside a markdown code block, which would cause an error if we tried to load it with `json.loads()`.\n",
    "\n",
    "We'll now learn a few techniques to help guarantee the model outputs valid JSON when we need it to. The two tools we'll use to accomplish this are:\n",
    "\n",
    "- adding instructions to the system prompt ensuring the output is JSON\n",
    "- enabling OpenAI's JSON Mode in the Chat Completions API\n",
    "\n",
    "While JSON Mode is a unique feature in the OpenAI Chat Completions API, other frontier models will also benefit from the system prompt technique we'll describe.\n",
    "\n",
    "If a system prompt is not available in your LLM of choice, you can also add these instructions to the main prompt body.\n",
    "\n",
    "Below is the same code as the call we executed above, with a few changes:\n",
    "\n",
    "- We've added a system prompt field instructing the model to output valid JSON without any markdown backticks\n",
    "- We've added an argument for `response_format` that sets the type of output to `json_object`.\n",
    "\n",
    "One important thing to note when specifying JSON object as the desired response format is that the prompt must contain \"JSON\" in the request or the call to the API will return an error. This is just an extra safeguard from OpenAI to ensure we get what we're asking for. Since we're including \"JSON\" in both our system prompt and user prompt, we'll be fine.\n",
    "\n",
    "Execute the code below and see if we get valid JSON in answer to our question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_demo_json_completion(system_prompt, prompt):\n",
    "    system_prompt = \"You are a helpful assistant designed to output JSON. Do not include any markdown backticks in your response.\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": prompt}],\n",
    "        seed=42,\n",
    "        response_format={\"type\": \"json_object\"}\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "answer = get_demo_json_completion('Who won the world series in 2020? Please respond in JSON, with \"winner\" as the key and the winner as the value.')\n",
    "json_answer = json.loads(answer)\n",
    "print(json_answer['winner'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success! Now you try. Below is a prompt that will not output valid JSON if sent to the model as-is. Make the same changes that we made to the prompt above so that it successfully parses and displays the data in the JSON.\n",
    "\n",
    "Execute the code below and see if we get valid JSON in answer to our question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint 1/3\n",
    "\n",
    "Add a system prompt instructing the model to output valid JSON without markdown backticks and add `\"json_object\"` as the type of `response_format` when the API is called. Then parse the JSON using `json.loads()`, assigning the result to the variable `json_output`."
   ]
  },
  {
   "cell_type": "code",
   "source": "def get_json_completion(prompt):\n    system_prompt = ## YOUR SOLUTION HERE ##\n    response = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": prompt}],\n        seed=42,\n        ## YOUR SOLUTION HERE ##\n    )\n    return response.choices[0].message.content\n\nanswer = get_json_completion(\"\"\"Output a JSON object with four keys: \"name\", \"age\", \"city\", and \"book\".\nMake their values the name, age, and city of a character from a book, and the book title.\"\"\")\njson_output = ## YOUR SOLUTION HERE ##\nprint('NAME: ', json_output['name'])\nprint('AGE: ', json_output['age'])\nprint('CITY: ', json_output['city'])\nprint('BOOK: ', json_output['book'])",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Now that we've got the basics down, let's explore some ways to use this tool to enable interesting structures for agentic systems.\n\nThe first such structure we'll explore involves prompting items in a list. Imagine you want the model to generate a list of ideas for possible blog posts, with a draft of a 'hook' paragraph for each. Instead of attempting to do it all in one prompt—which may degrade quality, as the model is forced to attend to many different ideas at once—you can iterate across a list of ideas and prompt the model for only one draft at a time. Let's try it out.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Checkpoint 2/3\n\nYour task will be to complete the function `generate_blog_ideas` and correctly parse and prepare its result. The function will be used to generate valid JSON in the form of `{\"data\": [\"list\", \"of\", \"blog ideas\"]}`.\n\nWhen the list of ideas is successfully extracted and assigned to `ideas_list`, we iterate through each item in that list, prompting the model for a 'hook' paragraph to start off a blog post about that topic. Note that you are expected to pass your own topic to the `generate_blog_ideas` function call. Make it a short string of one or two words.\n\nBe sure to instruct the model to output JSON in the system prompt and set the `response_format` parameter to `{\"type\": \"json_object\"}`. Complete the messages list such that the system prompt and prompt are both passed into their appropriate positions.\n\nThen, pass your own blog topic to the `generate_blog_ideas` function. Parse the JSON using the method we used earlier. And extract the value of the `\"data\"` key and assign it to `ideas_list`. Then execute the function and see the model write the hooks for three different blog post ideas.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def generate_blog_ideas(topic):\n    system_prompt =\n    prompt = f\"\"\"\n    <instructions>Generate a list of 3 blog post ideas related to the topic of '{topic}'.</instructions>\n    <format>Each idea should be a string in the JSON field 'data'.</format>\n    \"\"\"\n    response = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"system\", \"content\": }, ## YOUR SOLUTION HERE ##\n                  {\"role\": \"user\", \"content\": }], ## YOUR SOLUTION HERE ##\n        seed=42,\n        response_format={\"type\": \"json_object\"})\n    \n    return response.choices[0].message.content\n\nideas = generate_blog_ideas() ## YOUR SOLUTION HERE ##\njson_ideas = ## YOUR SOLUTION HERE ##\nideas_list = ## YOUR SOLUTION HERE ##\n\nfor idea in ideas_list:\n    draft = f\"\"\"\n    <instructions>Generate the opening \"hook\" paragraph for a blog post about the following idea:</instructions>\n    <style>\n    The hook should be a single paragraph that is no more than 100 words. It should be written in a way that is engaging and interesting to the reader.\n    Write at an eighth grade reading level.\n    </style>\n    <idea>{idea}</idea>\n    \"\"\"\n    draft_prompt = f\"<topic>{idea}: {idea}\"\n    draft = get_completion(draft_prompt)\n    print(draft)\n    print('---')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Lists aren't the only way to interact with JSON data in agentically interesting ways. In our final checkpoint, let's see what dictionaries can accomplish.\n\nOne reliable use case of LLMs is in condensing and adding structure to large texts. Let's use JSON Mode to extract structured data from an email text. We'll then format the resulting dictionary to prepare it for upload into a database.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Checkpoint 3/3\n\nThis exercise will be a little more open-ended. Given an email text, write a prompt that will extract data from the email and assign it to the variable `email_data`. The data must be in the following shape:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "email_data = {\n    \"customer_info\": {\n        \"name\": \"name from email here\",\n        \"customer_id\": \"id from email here\",\n        \"contact\": {\n            \"email\": \"email address from email here\",\n            \"phone\": \"phone number from email here\"\n        }\n    },\n    \"email_summary\": \"email summary here\",\n    \"order_details\": {\n        \"order_number\": \"order number from email here\",\n        \"issues\": [\"list\", \"of\", \"everything wrong with order here\"]\n    }\n}",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "We'll provide the `email_text` below and you'll write the code to extract this data via the LLM. Be sure to assign the resulting Python dictionary to `email_data` at the bottom of the cell.\n\nOne technique that will immensely help your outputs here is demonstrating to the model the shape of the JSON object you expect to be returned. Remember that curly braces can be included in f-strings by doubling them up. For example:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "json_f_string = f\"\"\"\nMake your output look like this:\n{{\n    \"data\": \"big json object\"\n}}\n\"\"\"",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "email_text = \"\"\"\nSubject: Urgent Issue with Recent Order #45789 and Website Navigation Problems\n\nDear Customer Support Team,\n\nI hope this email finds you well. My name is Sarah Johnson, and I'm writing to express my frustration with several issues I've encountered with my recent order and your website.\n\nFirst, I placed order #45789 on March 15th for the Premium XL Blender (Model BX2000) in blue, but I received the standard model in red instead.\n\nSecond, I've been trying to access my account on your website for the past three days, but I keep getting an error message saying \"Server timeout\" whenever I try to log in.\n\nI also noticed that your company website has a slight typo on the homepage - it says \"Our prodcuts are the best\" instead of \"Our products are the best.\" The navigation menu seems broken too.\n\nOn a slightly more positive note, I did appreciate the quick email response from your automated system acknowledging my order. The tracking information was helpful while it worked.\n\nI've been a loyal customer for over five years and have recommended your products to many friends and family members. However, this experience has been disappointing.\n\nPlease address these issues as soon as possible. I expect either the correct product to be shipped within the next week or a full refund including shipping costs.\n\nIf I don't hear back within 48 hours, I will unfortunately need to contact my credit card company to dispute the charges and file a complaint with the Better Business Bureau.\n\nThank you for your prompt attention to this matter.\n\nSincerely,\nSarah Johnson\nCustomer ID: J29875\nPhone: (555) 123-4567\nEmail: sarah.johnson@email.com\n\"\"\"",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def get_json_completion(prompt):\n    system_prompt = \"You are a helpful assistant that outputs JSON. Do not include any markdown backticks in your response.\"\n    response = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": prompt}],\n        seed=42,\n        response_format={\"type\": \"json_object\"})\n    \n    return response.choices[0].message.content\n\n# Create a prompt that asks the LLM to extract structured data from the email\nprompt = f\"\"\"\n## YOUR SOLUTION HERE ##\n\"\"\"\n\n# Get the JSON response from the LLM\njson_response = get_json_completion(prompt)\n\n# Parse the JSON string into a Python dictionary\nemail_data = json.loads(json_response)\nprint(email_data)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}